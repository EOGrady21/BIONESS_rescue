---
title: "Historical BIONESS Archival"
output: html_notebook
author: "Emily O'Grady"
---
This R markdown outlines the steps taken to archive historical BIONESS data in BioChem.
This project was undertaken under the direction of Catherine Johnson at Fisheries and Oceans
Canada, Maritimes Region Ocean Ecosystems Science Division.

This report summarizes the completion of the project involving the consolidation and quality control of plankton data from various missions. The data was consolidated from various locations and shared folders at Bedford Institute of Oceanography from missions between 2009 and 2016. 

# Data cleaning and formatting

The raw data was consolidated into a single folder for each cruise, sourced from various locations. We checked the plankton data loaded to BioChem for each mission, noting that none included BIONESS data. Metadata was checked for each mission, and BIONESS log sheets were found for each mission since most missions required at least some metadata from the logs to be input.

A discussion with Rebecca Milne prompted the organization of data accounting by individual station, as data was often analyzed by location rather than mission. For example, Gully tows from 2003-2010 were analyzed together, likely due to funding organization. We reviewed log sheets and set up an accounting spreadsheet for 2009-2016 data by mission and station. 

* How many missions and stations were archived as part of this project* 

* Table of data included *



# Metadata cleaning and supplementing

The process of metadata cleaning and supplementing was a crucial step in ensuring the integrity and usability of the plankton data for future analyses.

### Initial Assessment and Consolidation

The first step involved consolidating raw data from various sources into a single folder for each cruise. This included data from Shelley Bond's investigative folders (from Mary Kennedy's archives) and SRC cruise folders. We then conducted an initial assessment of the metadata to identify gaps and inconsistencies.

### Identifying Metadata Gaps

During the initial assessment, we identified several gaps and inconsistencies in the metadata. Common issues included missing sample IDs, sounding data, and discrepancies in date formats. Sounding data, typically recorded on BIONESS log sheets, was often missing and needed to be entered manually. Sample IDs were inconsistently recorded,and stickers were not used on some missions, required compound unique sample IDs to be generated. In those cases (*list missions*), the formaula used to create compound sample IDs followed Mary Kennedy's procdures: *compound sample ID formula*

### Standardizing Metadata

To address these issues, we standardized the metadata across all files. This involved:

Ensuring consistent date formats (DD-MON-YY).
Standardizing column names and metadata formatting, including dates and gear representation.
Adding missing metadata manually from handwritten log sheets, such as event numbers, dates, and sample IDs.

### Quality Control and Validation

Quality control was a critical part of the metadata cleaning process. We conducted thorough checks to ensure:

- Dates matched between log sheets and data.
- Start and end depths were consistent between log sheets and data.
- Tow-sample matches were accurate.
- Event numbers were consistent across log sheets and data.
- Results of these checks were recorded in a tracking spreadsheet, and any inconsistencies were documented in a metadata discrepancies file.

### Supplementing Metadata

In cases where metadata was missing or incomplete, we supplemented it using various methods:

- Filling in sample IDs using a compound ID formula where necessary.
- Confirming metadata with team members, such as checking what was written on sample bottles when stickers were not used.

In some older missions (pre 2014), there were no elog or equivalent files. In these cases, elog tables were manually generated from handwritten bridge log and BIONESS log sheet information. These elog files match the format used to load elog metadata into the AZMP template. 


# Load file formats
The standardized format of the files is essential for ensuring consistency and ease of use in future analyses. Each file follows a specific structure with clearly defined columns. Below is an overview of the file format.

Standardized File Structure
Each file contains the following columns:

* format into table*
Column Name	Definition
MISSION	Provide definition here
DATE	Provide definition here
STN	Provide definition here
TOW#	Provide definition here
GEAR	Provide definition here
EVENT	Provide definition here
SAMPLEID	Provide definition here
START_DEPTH	Provide definition here
END_DEPTH	Provide definition here
ANALYSIS	Provide definition here
SPLIT	Provide definition here
ALIQUOT	Provide definition here
SPLIT_FRACTION	Provide definition here
TAXA	Provide definition here
NCODE	Provide definition here
STAGE	Provide definition here
SEX	Provide definition here
DATA_VALUE	Provide definition here
PROC_CODE	Provide definition here
WHAT_WAS_IT	Provide definition here
COMMENT	Provide definition here

### Date Format

The date format used in all files is DD-MON-YY to ensure consistency across datasets.

# Extracting volume data from electronic files

In order to properly represent the plankton data in BioChem, the sampled volume
information was essential. This metadata was found in the BIONESS electronic files
which were located for all except 1 historical mission slated to be archived.

The elctronic file format was confirmed with Nelson Rice who wrote the original
program to produce the files. 

The ectronic file log was generated manually with information from the 
logsheets, electronic files and taxonomic data. It includes columns
  - mission
  - tow
  - event
  - electronic_file_name
  - sample_id
  - net_number
  - volume

```{r}
library(BIONESSQC)
datapath <- 'R:/Science/BIODataSvc/SRC/ZPlankton_DataRescue/OGrady_2024/raw_data'
# Gather all files for HUD2014030 (test mission to be loaded first)
efiles <- readxl::read_xlsx(file.path(datapath, 'HUD2014030/HUD2014030_electronic_file_log.xlsx'))
efile_paths <- list()
for (i in 1:length(efiles$electronic_file_name)) {
  efile_paths[[i]] <- list.files(file.path(datapath, efiles$mission[i]),
             pattern = efiles$electronic_file_name[i],
             recursive = TRUE,
             full.names = TRUE)
}

bionessdata <- list()
for (i in 1:length(efile_paths)) {
  bionessdata[[i]] <- read_bioness(efile_paths[[i]])
}

```


Once the electronic files were accessible in R, I was able to extract the volume 
data to provide along with the plankton taxonomic data and other event metadata.

```{r}
# Read the volume table
vol_table <- readxl::read_xlsx(file.path(datapath, 'HUD2014030/HUD2014030_electronic_file_log.xlsx'))

# Synthesize volume data in a table for loading
vol_full <- map_dfr(bionessdata, function(efiledat) {
  event_num <- as.numeric(efiledat$metadata$`Event #`)
  net_nums <- as.numeric(str_extract(efiledat$data$net_number, "\\d+"))
  volume <- as.numeric(efiledat$data$volume)
  
  vol_dat <- data.frame(event_num, net_nums, volume)
  
  vol_table %>%
    select(-volume) %>%
    inner_join(vol_dat, by = c('event' = 'event_num', 'net_number' = 'net_nums'))
})

xlsx::write.xlsx(x = vol_full,
                 file = file.path(datapath, 'HUD2014030/HUD2014030_electronic_file_log.xlsx')
                 )

head(vol_full)
```

Volume data is combined with other metadata including event, and sample ID to be
matched up to taxonomic data.


# Quality Control

*List tests and what they check along with results*

# Conclusion

*Status of BIONESS data, what other data exists which was outside of this project scope*
*Links to github and data*


